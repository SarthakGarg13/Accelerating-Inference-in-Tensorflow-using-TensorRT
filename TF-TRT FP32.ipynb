{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm \n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DownloadProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "\"\"\" \n",
    "    check if the data (zip) file is already downloaded\n",
    "    if not, download it from \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\" and save as cifar-10-python.tar.gz\n",
    "\"\"\"\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DownloadProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_names():\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        # note the encoding type is 'latin1'\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: input image data in numpy array [32, 32, 3]\n",
    "        return\n",
    "            - normalized x \n",
    "    \"\"\"\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "    x = (x-min_val) / (max_val-min_val)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "        argument\n",
    "            - x: a list of labels\n",
    "        return\n",
    "            - one hot encoding matrix (number of labels, number of class)\n",
    "    \"\"\"\n",
    "    encoded = np.zeros((len(x), 10))\n",
    "    \n",
    "    for idx, val in enumerate(x):\n",
    "        encoded[idx][val] = 1\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
    "    features = normalize(features)\n",
    "    labels = one_hot_encode(labels)\n",
    "\n",
    "    pickle.dump((features, labels), open(filename, 'wb'))\n",
    "\n",
    "\n",
    "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
    "    n_batches = 5\n",
    "    valid_features = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for batch_i in range(1, n_batches + 1):\n",
    "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
    "        \n",
    "        # find index to be the point as validation data in the whole dataset of the batch (10%)\n",
    "        index_of_validation = int(len(features) * 0.1)\n",
    "\n",
    "        # preprocess the 90% of the whole dataset of the batch\n",
    "        # - normalize the features\n",
    "        # - one_hot_encode the lables\n",
    "        # - save in a new file named, \"preprocess_batch_\" + batch_number\n",
    "        # - each file for each batch\n",
    "        _preprocess_and_save(normalize, one_hot_encode,\n",
    "                             features[:-index_of_validation], labels[:-index_of_validation], \n",
    "                             'preprocess_batch_' + str(batch_i) + '.p')\n",
    "\n",
    "        # unlike the training dataset, validation dataset will be added through all batch dataset\n",
    "        # - take 10% of the whold dataset of the batch\n",
    "        # - add them into a list of\n",
    "        #   - valid_features\n",
    "        #   - valid_labels\n",
    "        valid_features.extend(features[-index_of_validation:])\n",
    "        valid_labels.extend(labels[-index_of_validation:])\n",
    "\n",
    "    # preprocess the all stacked validation dataset\n",
    "    _preprocess_and_save(normalize, one_hot_encode,\n",
    "                         np.array(valid_features), np.array(valid_labels),\n",
    "                         'preprocess_validation.p')\n",
    "\n",
    "    # load the test dataset\n",
    "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    # preprocess the testing data\n",
    "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    test_labels = batch['labels']\n",
    "\n",
    "    # Preprocess and Save all testing data\n",
    "    _preprocess_and_save(normalize, one_hot_encode,\n",
    "                         np.array(test_features), np.array(test_labels),\n",
    "                         'preprocess_training.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous weights, bias, inputs, etc..\n",
    "import tensorflow as  tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "# keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "# keep_prob=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def conv_net(x):\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "    conv3_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.08))\n",
    "    conv4_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 256, 512], mean=0, stddev=0.08))\n",
    "\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool,name='input')\n",
    "    print(conv1_bn)\n",
    "   \n",
    "\n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "  \n",
    "    # 5, 6\n",
    "    conv3 = tf.nn.conv2d(conv2_bn, conv3_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  \n",
    "    conv3_bn = tf.layers.batch_normalization(conv3_pool)\n",
    "    \n",
    "    # 7, 8\n",
    "    conv4 = tf.nn.conv2d(conv3_bn, conv4_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "    conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv4_bn = tf.layers.batch_normalization(conv4_pool)\n",
    "    \n",
    "    # 9\n",
    "    flat = tf.contrib.layers.flatten(conv4_bn)  \n",
    "\n",
    "    # 10\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "#     full1 = tf.nn.dropout(full1, keep_prob)\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "    # 11\n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "#     full2 = tf.nn.dropout(full2, keep_prob)\n",
    "    full2 = tf.layers.batch_normalization(full2)\n",
    "    \n",
    "    # 12\n",
    "    full3 = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=tf.nn.relu)\n",
    "#     full3 = tf.nn.dropout(full3, keep_prob)\n",
    "    full3 = tf.layers.batch_normalization(full3)    \n",
    "    \n",
    "    # 13\n",
    "    full4 = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=1024, activation_fn=tf.nn.relu)\n",
    "#     full4 = tf.nn.dropout(full4, keep_prob)\n",
    "    full4 = tf.layers.batch_normalization(full4)        \n",
    "    \n",
    "    # 14\n",
    "    out = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=10, activation_fn=None)\n",
    "    print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 128\n",
    "# keep_probability = 0.7\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input/FusedBatchNorm:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"fully_connected_4/BiasAdd:0\", shape=(?, 10), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-14-ed7be955df3a>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = conv_net(x)\n",
    "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, feature_batch, label_batch):\n",
    "    session.run(optimizer, \n",
    "                feed_dict={\n",
    "                    x: feature_batch,\n",
    "                    y: label_batch,\n",
    "#                     keep_prob: keep_probability\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    loss = sess.run(cost, \n",
    "                    feed_dict={\n",
    "                        x: feature_batch,\n",
    "                        y: label_batch,\n",
    "#                         keep_prob: 0.7\n",
    "                    })\n",
    "    valid_acc = sess.run(accuracy, \n",
    "                         feed_dict={\n",
    "                             x: valid_features,\n",
    "                             y: valid_labels,\n",
    "#                              keep_prob: 0.7\n",
    "                         })\n",
    "    \n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]\n",
    "\n",
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, mode='rb'))\n",
    "\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return batch_features_labels(features, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.3072 Validation Accuracy: 0.105000\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.2769 Validation Accuracy: 0.094200\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.2950 Validation Accuracy: 0.094600\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.3026 Validation Accuracy: 0.094200\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.3010 Validation Accuracy: 0.105000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.3017 Validation Accuracy: 0.099800\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.2779 Validation Accuracy: 0.094200\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     2.2916 Validation Accuracy: 0.094200\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     2.3064 Validation Accuracy: 0.099800\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     2.3006 Validation Accuracy: 0.105000\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, batch_features, batch_labels)\n",
    "                \n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, './model/tensorflow/cifar/cnn_model')\n",
    "    print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/tensorflow/cifar/cnn_model\n",
      "INFO:tensorflow:Froze 40 variables.\n",
      "INFO:tensorflow:Converted 40 variables to const ops.\n",
      "Frozen model is successfully stored!\n"
     ]
    }
   ],
   "source": [
    "# import the needed libraries\n",
    "import tensorflow.contrib.tensorrt as trt\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "\n",
    "# has to be use this setting to make a session for TensorRT optimization\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
    "    # import the meta graph of the tensorflow model\n",
    "    #saver = tf.train.import_meta_graph(\"./model/tensorflow/big/model1.meta\")\n",
    "    saver = tf.train.import_meta_graph(\"./model/tensorflow/cifar/cnn_model.meta\")\n",
    "    # then, restore the weights to the meta graph\n",
    "    #saver.restore(sess, \"./model/tensorflow/big/model1\")\n",
    "    saver.restore(sess, \"./model/tensorflow/cifar/cnn_model\")\n",
    "    \n",
    "    # specify which tensor output you want to obtain \n",
    "    # (correspond to prediction result)\n",
    "    your_outputs = [\"logits\"]\n",
    "    \n",
    "    # convert to frozen model\n",
    "    frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "        sess, # session\n",
    "        tf.get_default_graph().as_graph_def(),# graph+weight from the session\n",
    "        output_node_names=your_outputs)\n",
    "    #write the TensorRT model to be used later for inference\n",
    "    with gfile.FastGFile(\"./model/cnn_model.pb\", 'wb') as f:\n",
    "        f.write(frozen_graph.SerializeToString())\n",
    "    print(\"Frozen model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running against TensorRT version 0.0.0\n",
      "TensorRT model is successfully stored!\n"
     ]
    }
   ],
   "source": [
    "# convert (optimize) frozen model to TensorRT model\n",
    "trt_graph = trt.create_inference_graph(\n",
    "    input_graph_def=frozen_graph,# frozen model\n",
    "    outputs=your_outputs,\n",
    "    max_batch_size=32,# specify your max batch size\n",
    "    max_workspace_size_bytes=2*(10**9),# specify the max workspace\n",
    "    precision_mode=\"FP32\") # precision, can be \"FP32\" (32 floating point precision) or \"FP16\"\n",
    "\n",
    "#write the TensorRT model to be used later for inference\n",
    "with gfile.FastGFile(\"./model/TensorRTCNN.pb\", 'wb') as f:\n",
    "    f.write(trt_graph.SerializeToString())\n",
    "print(\"TensorRT model is successfully stored!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numb. of all_nodes in frozen graph: 141\n",
      "numb. of trt_engine_nodes in TensorRT graph: 0\n",
      "numb. of all_nodes in TensorRT graph: 78\n"
     ]
    }
   ],
   "source": [
    "# check how many ops of the original frozen model\n",
    "all_nodes = len([1 for n in frozen_graph.node])\n",
    "print(\"numb. of all_nodes in frozen graph:\", all_nodes)\n",
    "\n",
    "# check how many ops that is converted to TensorRT engine\n",
    "trt_engine_nodes = len([1 for n in trt_graph.node if str(n.op) == 'TRTEngineOp'])\n",
    "print(\"numb. of trt_engine_nodes in TensorRT graph:\", trt_engine_nodes)\n",
    "all_nodes = len([1 for n in trt_graph.node])\n",
    "print(\"numb. of all_nodes in TensorRT graph:\", all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the needed libraries\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.tensorrt as trt # must import this although we will not use it explicitly\n",
    "from tensorflow.python.platform import gfile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# read the testing images (only for example)\n",
    "img1= Image.open(\"dataset/cifar/testing/img_108.jpg\")\n",
    "img2= Image.open(\"dataset/cifar/testing/img_0.jpg\")\n",
    "img1 = np.asarray(img1)\n",
    "img2 = np.asarray(img2)\n",
    "img1=np.resize(img1,(1,32,32,3))\n",
    "img2=np.resize(img2,(1,32,32,3))\n",
    "input_img = np.concatenate((img1.reshape((1, 32, 32, 3)), \n",
    "                            img2.reshape((1, 32, 32, 3))), \n",
    "                           axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read a \".pb\" model \n",
    "# (can be used to read frozen model or TensorRT model)\n",
    "def read_pb_graph(model):\n",
    "  with gfile.FastGFile(model,'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "  return graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needed time in inference-0:  0.027825355529785156\n",
      "needed time in inference-1:  0.03010082244873047\n",
      "needed time in inference-2:  0.02129340171813965\n",
      "needed time in inference-3:  0.027138233184814453\n",
      "needed time in inference-4:  0.021171092987060547\n",
      "needed time in inference-5:  0.027632713317871094\n",
      "needed time in inference-6:  0.021397829055786133\n",
      "needed time in inference-7:  0.0277407169342041\n",
      "needed time in inference-8:  0.02151203155517578\n",
      "needed time in inference-9:  0.027474164962768555\n",
      "needed time in inference-10:  0.021320104598999023\n",
      "needed time in inference-11:  0.028126001358032227\n",
      "needed time in inference-12:  0.021244049072265625\n",
      "needed time in inference-13:  0.027773141860961914\n",
      "needed time in inference-14:  0.027821063995361328\n",
      "needed time in inference-15:  0.021266698837280273\n",
      "needed time in inference-16:  0.027657032012939453\n",
      "needed time in inference-17:  0.02158212661743164\n",
      "needed time in inference-18:  0.027487993240356445\n",
      "needed time in inference-19:  0.02165985107421875\n",
      "needed time in inference-20:  0.027766704559326172\n",
      "needed time in inference-21:  0.027659893035888672\n",
      "needed time in inference-22:  0.023893356323242188\n",
      "needed time in inference-23:  0.02131509780883789\n",
      "needed time in inference-24:  0.027734756469726562\n",
      "needed time in inference-25:  0.021533966064453125\n",
      "needed time in inference-26:  0.021313190460205078\n",
      "needed time in inference-27:  0.02829122543334961\n",
      "needed time in inference-28:  0.021152973175048828\n",
      "needed time in inference-29:  0.02096867561340332\n",
      "needed time in inference-30:  0.02075934410095215\n",
      "needed time in inference-31:  0.027726411819458008\n",
      "needed time in inference-32:  0.02141094207763672\n",
      "needed time in inference-33:  0.021245241165161133\n",
      "needed time in inference-34:  0.02768564224243164\n",
      "needed time in inference-35:  0.021396160125732422\n",
      "needed time in inference-36:  0.027535676956176758\n",
      "needed time in inference-37:  0.021335601806640625\n",
      "needed time in inference-38:  0.02770376205444336\n",
      "needed time in inference-39:  0.02188849449157715\n",
      "needed time in inference-40:  0.027692556381225586\n",
      "needed time in inference-41:  0.02148580551147461\n",
      "needed time in inference-42:  0.021532773971557617\n",
      "needed time in inference-43:  0.027561426162719727\n",
      "needed time in inference-44:  0.021589994430541992\n",
      "needed time in inference-45:  0.028033971786499023\n",
      "needed time in inference-46:  0.021210908889770508\n",
      "needed time in inference-47:  0.027905702590942383\n",
      "needed time in inference-48:  0.021461963653564453\n",
      "needed time in inference-49:  0.027721405029296875\n",
      "average inference time:  0.024514760971069336\n"
     ]
    }
   ],
   "source": [
    "# variable\n",
    "TENSORRT_MODEL_PATH = './model/TensorRTCNN.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
    "        # read TensorRT model\n",
    "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(trt_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('input_x:0')\n",
    "        output = sess.graph.get_tensor_by_name('fully_connected_4/BiasAdd:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 50\n",
    "        out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "        avg_time_tensorRT = total_time / n_time_inference\n",
    "        print(\"average inference time: \", avg_time_tensorRT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needed time in inference-0:  0.023161649703979492\n",
      "needed time in inference-1:  0.028136253356933594\n",
      "needed time in inference-2:  0.027724027633666992\n",
      "needed time in inference-3:  0.021545886993408203\n",
      "needed time in inference-4:  0.027658700942993164\n",
      "needed time in inference-5:  0.02146458625793457\n",
      "needed time in inference-6:  0.021537065505981445\n",
      "needed time in inference-7:  0.021086454391479492\n",
      "needed time in inference-8:  0.02771162986755371\n",
      "needed time in inference-9:  0.021712064743041992\n",
      "needed time in inference-10:  0.02820587158203125\n",
      "needed time in inference-11:  0.02153635025024414\n",
      "needed time in inference-12:  0.02823472023010254\n",
      "needed time in inference-13:  0.021699190139770508\n",
      "needed time in inference-14:  0.021615028381347656\n",
      "needed time in inference-15:  0.027460575103759766\n",
      "needed time in inference-16:  0.021802425384521484\n",
      "needed time in inference-17:  0.021591663360595703\n",
      "needed time in inference-18:  0.02849292755126953\n",
      "needed time in inference-19:  0.021890640258789062\n",
      "needed time in inference-20:  0.021015167236328125\n",
      "needed time in inference-21:  0.03075885772705078\n",
      "needed time in inference-22:  0.028804779052734375\n",
      "needed time in inference-23:  0.02208709716796875\n",
      "needed time in inference-24:  0.02843189239501953\n",
      "needed time in inference-25:  0.02161264419555664\n",
      "needed time in inference-26:  0.021088361740112305\n",
      "needed time in inference-27:  0.028293848037719727\n",
      "needed time in inference-28:  0.02437758445739746\n",
      "needed time in inference-29:  0.021426677703857422\n",
      "needed time in inference-30:  0.02812361717224121\n",
      "needed time in inference-31:  0.021535158157348633\n",
      "needed time in inference-32:  0.028025150299072266\n",
      "needed time in inference-33:  0.021472930908203125\n",
      "needed time in inference-34:  0.027997970581054688\n",
      "needed time in inference-35:  0.02147650718688965\n",
      "needed time in inference-36:  0.02800130844116211\n",
      "needed time in inference-37:  0.021324634552001953\n",
      "needed time in inference-38:  0.027870655059814453\n",
      "needed time in inference-39:  0.02156805992126465\n",
      "needed time in inference-40:  0.021436691284179688\n",
      "needed time in inference-41:  0.028317928314208984\n",
      "needed time in inference-42:  0.028273820877075195\n",
      "needed time in inference-43:  0.021527528762817383\n",
      "needed time in inference-44:  0.02839970588684082\n",
      "needed time in inference-45:  0.022318124771118164\n",
      "needed time in inference-46:  0.028188467025756836\n",
      "needed time in inference-47:  0.021560192108154297\n",
      "needed time in inference-48:  0.02816009521484375\n",
      "needed time in inference-49:  0.022031784057617188\n",
      "average inference time:  0.02459549903869629\n"
     ]
    }
   ],
   "source": [
    "# variable\n",
    "FROZEN_MODEL_PATH = './model/cnn_model.pb'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        # read TensorRT model\n",
    "        frozen_graph = read_pb_graph(FROZEN_MODEL_PATH)\n",
    "\n",
    "        # obtain the corresponding input-output tensor\n",
    "        tf.import_graph_def(frozen_graph, name='')\n",
    "        input = sess.graph.get_tensor_by_name('input_x:0')\n",
    "        output = sess.graph.get_tensor_by_name('fully_connected_4/BiasAdd:0')\n",
    "\n",
    "        # in this case, it demonstrates to perform inference for 50 times\n",
    "        total_time = 0; n_time_inference = 50\n",
    "        out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "        for i in range(n_time_inference):\n",
    "            t1 = time.time()\n",
    "            out_pred = sess.run(output, feed_dict={input: input_img})\n",
    "            t2 = time.time()\n",
    "            delta_time = t2 - t1\n",
    "            total_time += delta_time\n",
    "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
    "        avg_time_original_model = total_time / n_time_inference\n",
    "        print(\"average inference time: \", avg_time_original_model)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT improvement compared to the original model: 1.00329344706735\n"
     ]
    }
   ],
   "source": [
    " print(\"TensorRT improvement compared to the original model:\", avg_time_original_model/avg_time_tensorRT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
